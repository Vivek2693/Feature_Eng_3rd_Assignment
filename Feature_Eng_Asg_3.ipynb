{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cf4b916",
   "metadata": {},
   "source": [
    "### Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.\n",
    "\n",
    "Min-Max scaling is a technique used to transform features by scaling them to a specified range, typically between 0 and 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1f06407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.  0. ]\n",
      " [0.5 0.5]\n",
      " [1.  1. ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "data = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "\n",
    "# Instantiate MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform data\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "print(scaled_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7f3f45",
   "metadata": {},
   "source": [
    "### Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application.\n",
    "\n",
    "A2: The Unit Vector technique, also known as normalization, scales individual samples to have unit norm (i.e., length of 1). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848b3ddb",
   "metadata": {},
   "source": [
    "X_Normalized = X/||X||\n",
    "\n",
    " This technique scales each sample independently to have a unit norm, ensuring that the features are on the same scale but preserving the direction of the data. Unlike Min-Max scaling, normalization does not scale the features to a predefined range but rather ensures that each sample has a length of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cad73f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4472136  0.89442719]\n",
      " [0.6        0.8       ]\n",
      " [0.6401844  0.76822128]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "data = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "\n",
    "# Instantiate Normalizer\n",
    "normalizer = Normalizer()\n",
    "\n",
    "# Fit and transform data\n",
    "normalized_data = normalizer.fit_transform(data)\n",
    "\n",
    "print(normalized_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8685da",
   "metadata": {},
   "source": [
    "### Q3. What is PCA (Principal Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6d4319",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) is a dimensionality reduction technique that transforms the original features into a new set of orthogonal (uncorrelated) features called principal components. These principal components are ordered by the amount of variance they explain in the data, with the first component explaining the most variance.\n",
    "\n",
    "PCA works by identifying the directions (principal axes) in which the data varies the most and projecting the data onto these axes. This reduces the dimensionality of the data while preserving most of its variance, allowing for simpler and more efficient models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bed1ef0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5.19615242e+00  3.62353582e-16]\n",
      " [ 0.00000000e+00  0.00000000e+00]\n",
      " [ 5.19615242e+00  3.62353582e-16]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "# Instantiate PCA with 2 components\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit and transform data\n",
    "transformed_data = pca.fit_transform(data)\n",
    "\n",
    "print(transformed_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50370a75",
   "metadata": {},
   "source": [
    "### Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ce9d49",
   "metadata": {},
   "source": [
    "|PCA can be used for Feature Extraction by transforming the original features into a new set of principal components. These principal components are linear combinations of the original features and represent directions in the feature space where the data varies the most. By selecting a subset of the principal components, we can effectively extract the most informative features from the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad41c97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5.19615242e+00  3.62353582e-16]\n",
      " [ 0.00000000e+00  0.00000000e+00]\n",
      " [ 5.19615242e+00  3.62353582e-16]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "# Instantiate PCA with 2 components\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit and transform data\n",
    "transformed_data = pca.fit_transform(data)\n",
    "\n",
    "print(transformed_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf80e6d5",
   "metadata": {},
   "source": [
    "### Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "raw",
   "id": "747566f8",
   "metadata": {},
   "source": [
    "To use Min-Max scaling for preprocessing the data:\n",
    "\n",
    "1.Select the features to be scaled, such as price, rating, and delivery time.\n",
    "2.Compute the minimum and maximum values for each selected feature in the dataset.\n",
    "3.Apply the Min-Max scaling formula to scale each feature to the range [0, 1].\n",
    "4.Use the scaled features as input to train the recommendation system model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c628945",
   "metadata": {},
   "source": [
    "### Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea175f3",
   "metadata": {},
   "source": [
    "In the context of predicting stock prices, PCA can be used to reduce the dimensionality of the dataset by identifying the most important underlying factors driving the variability in stock prices. Here's how you would use PCA:\n",
    "\n",
    "Data Preparation: Gather the dataset containing features related to company financial data and market trends, such as earnings per share, price-to-earnings ratio, volume of trade, etc.\n",
    "\n",
    "Data Preprocessing: Standardize the features to have zero mean and unit variance. This step ensures that all features are on the same scale and prevents features with larger magnitudes from dominating the principal components.\n",
    "\n",
    "Apply PCA: Use PCA to transform the standardized dataset into its principal components. PCA will identify linear combinations of the original features that capture the maximum variance in the data.\n",
    "\n",
    "Select Number of Components: Determine the number of principal components to retain based on the cumulative explained variance ratio. Retain enough components to capture a significant portion of the variance in the dataset while reducing its dimensionality.\n",
    "\n",
    "Feature Reduction: Transform the dataset using the selected number of principal components. This reduces the dimensionality of the dataset while preserving most of the information.\n",
    "\n",
    "Model Training: Use the reduced-dimensional dataset as input to train the predictive model for stock price prediction.\n",
    "\n",
    "PCA helps in simplifying the dataset by reducing the number of features while retaining the most important information. It can uncover hidden patterns and relationships in the data, making it easier for the predictive model to learn and generalize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c729870d",
   "metadata": {},
   "source": [
    "### Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "raw",
   "id": "b1c266aa",
   "metadata": {},
   "source": [
    "To perform Min-Max scaling on the given dataset to transform the values to a range of -1 to 1, we can follow these steps:\n",
    "\n",
    "Compute the minimum and maximum values in the dataset.\n",
    "Use the Min-Max scaling formula to scale each value to the desired range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13de0615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Given dataset\n",
    "data = np.array([1, 5, 10, 15, 20])\n",
    "\n",
    "# Compute min and max values\n",
    "min_val = np.min(data)\n",
    "max_val = np.max(data)\n",
    "\n",
    "# Perform Min-Max scaling\n",
    "scaled_data = (data - min_val) / (max_val - min_val) * 2 - 1\n",
    "\n",
    "print(scaled_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06d3246",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
